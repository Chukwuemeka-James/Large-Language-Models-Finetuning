{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Introduction to Tokenization and Model Evaluation**\n",
        "\n",
        "This notebook explores two fundamental pillars of working with large language models (LLMs):\n",
        "\n",
        "#### **Part 1: Tokenization Demystified**\n",
        "Tokenization is how LLMs convert raw text into numerical representations. We'll investigate:\n",
        "- How different models (LLaMA 3.1, Phi-3, etc.) tokenize numbers differently\n",
        "- Why tokenization matters for numerical tasks like price prediction\n",
        "- The real-world impact: A model that splits \"1000\" into two tokens may struggle with arithmetic compared to one that treats it as a single unit\n",
        "\n",
        "#### **Part 2: Model Evaluation Essentials**\n",
        "We then evaluate our base model's price prediction capabilities by:\n",
        "1. Loading LLaMA 3.1 with 4-bit quantization (reducing memory usage by 8x)\n",
        "2. Creating a robust testing framework that measures:\n",
        "   - Absolute price errors (in USD)\n",
        "   - Relative accuracy (percentage differences)\n",
        "   - Squared log error (penalizing large deviations)\n",
        "3. Visualizing predictions vs. actual prices to identify systematic biases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Learning Objectives:**\n",
        "1. Compare tokenization behavior across different LLMs\n",
        "2. Evaluate base model performance on price prediction\n",
        "3. Understand quantitative evaluation metrics\n",
        "4. Visualize model performance vs ground truth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Install required packages (commented to prevent accidental execution)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q datasets requests torch peft bitsandbytes transformers trl accelerate sentencepiece tiktoken matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import with clear grouping\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# HuggingFace and Colab specific\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# PyTorch and Transformers\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from datasets import load_dataset, Dataset, DatasetDict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Model Selection**\n",
        "\n",
        "We'll compare these foundation models:\n",
        "1. **LLaMA 3.1-8B** (Meta)\n",
        "2. **Qwen2.5-7B** (Alibaba)\n",
        "3. **Gemma-2-9B** (Google)\n",
        "4. **Phi-3-medium** (Microsoft)\n",
        "\n",
        "Note the parameter counts:\n",
        "- Our base: 8B params (4-bit quantized)\n",
        "- GPT-4: ~1.8T params (1000x larger!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizers\n",
        "\n",
        "LLAMA_3_1 = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "QWEN_2_5 = \"Qwen/Qwen2.5-7B\"\n",
        "GEMMA_2 = \"google/gemma-2-9b\"\n",
        "PHI_3 = \"microsoft/Phi-3-medium-4k-instruct\"\n",
        "\n",
        "# Constants\n",
        "\n",
        "BASE_MODEL = LLAMA_3_1\n",
        "HF_USER = \"ed-donner\"\n",
        "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
        "MAX_SEQUENCE_LENGTH = 182\n",
        "QUANT_4_BIT = True\n",
        "\n",
        "GREEN = \"\\033[92m\"\n",
        "YELLOW = \"\\033[93m\"\n",
        "RED = \"\\033[91m\"\n",
        "RESET = \"\\033[0m\"\n",
        "COLOR_MAP = {\"red\":RED, \"orange\": YELLOW, \"green\": GREEN}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HuggingFace Login\n",
        "\n",
        "Required steps:\n",
        "1. Create account at https://huggingface.co\n",
        "2. Generate token at https://huggingface.co/settings/tokens\n",
        "3. Add to Colab secrets (Key icon → New secret) named 'HF_TOKEN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tokenizer Analysis\n",
        "\n",
        "Different models tokenize numbers differently, which affects numerical reasoning.\n",
        "Let's examine how each model handles number tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def investigate_tokenizer(model_name):\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    for number in [0, 1, 10, 100, 999, 1000]:\n",
        "        tokens = tokenizer.encode(str(number), add_special_tokens=False)\n",
        "        print(f\"{number:>4} → {tokens} (Length: {len(tokens)})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Now we will try this with each model: LLAMA_3_1, QWEN_2_5, GEMMA_2, PHI_3\n",
        "\n",
        "for model_name in MODELS.values():\n",
        "    investigate_tokenizer(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Loading Price Prediction Dataset**\n",
        "\n",
        "Features:\n",
        "- Product descriptions\n",
        "- Ground truth prices\n",
        "- Split into train/test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = load_dataset(DATASET_NAME)\n",
        "train = dataset['train']\n",
        "test = dataset['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Loading Base Model with Quantization**\n",
        "\n",
        "Using 4-bit quantization for memory efficiency:\n",
        "- Normal Float 4 (nf4) quantization\n",
        "- Double quantization for additional savings\n",
        "- bfloat16 compute dtype for stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## pick the right quantization\n",
        "\n",
        "if QUANT_4_BIT:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "  )\n",
        "else:\n",
        "  quant_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    bnb_8bit_compute_dtype=torch.bfloat16\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "print(f\"\\nMemory footprint: {base_model.get_memory_footprint() / 1e9:.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Price Extraction Utility\n",
        "def extract_price(response):\n",
        "    \"\"\"Extract numerical price from model response\"\"\"\n",
        "    if \"Price is $\" in response:\n",
        "        contents = response.split(\"Price is $\")[1]\n",
        "        contents = contents.replace(',', '').replace('$', '')\n",
        "        match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
        "        return float(match.group()) if match else 0\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "extract_price(\"Price is $999 blah blah so cheap\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prediction Function\n",
        "def model_predict(prompt):\n",
        "    \"\"\"Generate price prediction from product description\"\"\"\n",
        "    set_seed(42)  # For reproducibility\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
        "    outputs = base_model.generate(\n",
        "        inputs,\n",
        "        max_new_tokens=4,  # Limit to price prediction\n",
        "        attention_mask=attention_mask,\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0])\n",
        "    return extract_price(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_predict(test[0]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **Evaluation Metrics**\n",
        "\n",
        "We'll track:\n",
        "1. Absolute Error (USD difference)\n",
        "2. Squared Log Error (SLE) - Penalizes large relative errors\n",
        "3. Accuracy Categories:\n",
        "   - Green: Error < $40 or < 20% of true price\n",
        "   - Yellow: Error < $80 or < 40%\n",
        "   - Red: Larger errors\n",
        "\n",
        "\n",
        "## **Base Model Performance Evaluation**\n",
        "\n",
        "Testing the LLaMA 3.1 8B model on 250 test cases:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Tester:\n",
        "\n",
        "    def __init__(self, predictor, data, title=None, size=250):\n",
        "        self.predictor = predictor\n",
        "        self.data = data\n",
        "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
        "        self.size = size\n",
        "        self.guesses = []\n",
        "        self.truths = []\n",
        "        self.errors = []\n",
        "        self.sles = []\n",
        "        self.colors = []\n",
        "\n",
        "    def color_for(self, error, truth):\n",
        "        if error<40 or error/truth < 0.2:\n",
        "            return \"green\"\n",
        "        elif error<80 or error/truth < 0.4:\n",
        "            return \"orange\"\n",
        "        else:\n",
        "            return \"red\"\n",
        "\n",
        "    def run_datapoint(self, i):\n",
        "        datapoint = self.data[i]\n",
        "        guess = self.predictor(datapoint[\"text\"])\n",
        "        truth = datapoint[\"price\"]\n",
        "        error = abs(guess - truth)\n",
        "        log_error = math.log(truth+1) - math.log(guess+1)\n",
        "        sle = log_error ** 2\n",
        "        color = self.color_for(error, truth)\n",
        "        title = datapoint[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
        "        self.guesses.append(guess)\n",
        "        self.truths.append(truth)\n",
        "        self.errors.append(error)\n",
        "        self.sles.append(sle)\n",
        "        self.colors.append(color)\n",
        "        print(f\"{COLOR_MAP[color]}{i+1}: Guess: ${guess:,.2f} Truth: ${truth:,.2f} Error: ${error:,.2f} SLE: {sle:,.2f} Item: {title}{RESET}\")\n",
        "\n",
        "    def chart(self, title):\n",
        "        max_error = max(self.errors)\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        max_val = max(max(self.truths), max(self.guesses))\n",
        "        plt.plot([0, max_val], [0, max_val], color='deepskyblue', lw=2, alpha=0.6)\n",
        "        plt.scatter(self.truths, self.guesses, s=3, c=self.colors)\n",
        "        plt.xlabel('Ground Truth')\n",
        "        plt.ylabel('Model Estimate')\n",
        "        plt.xlim(0, max_val)\n",
        "        plt.ylim(0, max_val)\n",
        "        plt.title(title)\n",
        "        plt.show()\n",
        "\n",
        "    def report(self):\n",
        "        average_error = sum(self.errors) / self.size\n",
        "        rmsle = math.sqrt(sum(self.sles) / self.size)\n",
        "        hits = sum(1 for color in self.colors if color==\"green\")\n",
        "        title = f\"{self.title} Error=${average_error:,.2f} RMSLE={rmsle:,.2f} Hits={hits/self.size*100:.1f}%\"\n",
        "        self.chart(title)\n",
        "\n",
        "    def run(self):\n",
        "        self.error = 0\n",
        "        for i in range(self.size):\n",
        "            self.run_datapoint(i)\n",
        "        self.report()\n",
        "\n",
        "    @classmethod\n",
        "    def test(cls, function, data):\n",
        "        cls(function, data).run()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Tester.test(model_predict, test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
