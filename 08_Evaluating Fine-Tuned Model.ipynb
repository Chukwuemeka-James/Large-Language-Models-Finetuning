{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluating Our Fine-Tuned LLM for Product Price Prediction**\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Load and test fine-tuned QLoRA adapters\n",
    "2. Compare prediction methods (greedy vs weighted)\n",
    "3. Evaluate model performance quantitatively\n",
    "4. Visualize prediction accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Install required packages (commented to prevent accidental execution)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets peft requests torch bitsandbytes transformers trl accelerate sentencepiece matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import with clear grouping\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# HuggingFace and Colab specific\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "\n",
    "# PyTorch and Transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    set_seed\n",
    ")\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "Key settings for our evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Selection\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "PROJECT_NAME = \"pricer\"\n",
    "HF_USER = \"ed-donner\"\n",
    "\n",
    "# Fine-Tuned Model Details\n",
    "RUN_NAME = \"2024-09-13_13.04.39\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "FINETUNED_MODEL = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "REVISION = \"e8d637df551603dc86cd7a1598a8f44af4d7ae36\"  # Specific model version\n",
    "\n",
    "# Dataset\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "\n",
    "# Quantization\n",
    "QUANT_4_BIT = True\n",
    "\n",
    "# Evaluation\n",
    "TOP_K = 3  # Number of top predictions to consider\n",
    "TEST_SIZE = 250  # Number of test samples to evaluate\n",
    "\n",
    "# Console Colors\n",
    "GREEN = \"\\033[92m\"\n",
    "YELLOW = \"\\033[93m\"\n",
    "RED = \"\\033[91m\"\n",
    "RESET = \"\\033[0m\"\n",
    "COLOR_MAP = {\"red\": RED, \"orange\": YELLOW, \"green\": GREEN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace Login\n",
    "\n",
    "Required to access models and datasets:\n",
    "1. Create account at https://huggingface.co\n",
    "2. Generate token at https://huggingface.co/settings/tokens\n",
    "3. Add to Colab secrets (Key icon â†’ New secret) named 'HF_TOKEN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Test Dataset\n",
    "\n",
    "Our evaluation dataset contains:\n",
    "- Product descriptions\n",
    "- Ground truth prices\n",
    "- Matches training data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Fine-Tuned Model\n",
    "\n",
    "We load:\n",
    "1. Base LLaMA 3.1 model (quantized)\n",
    "2. Fine-tuned LoRA adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the right quantization (thank you Robert M. for spotting the bug with the 8 bit version!)\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and base model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model with PEFT\n",
    "if REVISION:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL, revision=REVISION)\n",
    "else:\n",
    "  fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nMemory footprint: {fine_tuned_model.get_memory_footprint() / 1e6:.1f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Price Prediction Methods\n",
    "\n",
    "We implement two approaches:\n",
    "1. **Greedy decoding**: Takes most likely next token\n",
    "2. **Weighted prediction**: Averages top K predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(response):\n",
    "    \"\"\"Extract numerical price from model response\"\"\"\n",
    "    if \"Price is $\" in response:\n",
    "        contents = response.split(\"Price is $\")[1].replace(',', '')\n",
    "        match = re.search(r\"[-+]?\\d*\\.\\d+|\\d+\", contents)\n",
    "        return float(match.group()) if match else 0\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_price(\"Price is $a fabulous 899.99 or so\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_predict(prompt):\n",
    "    \"\"\"Standard greedy decoding prediction\"\"\"\n",
    "    set_seed(42)  # For reproducibility\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    attention_mask = torch.ones(inputs.shape, device=\"cuda\")\n",
    "    outputs = fine_tuned_model.generate(\n",
    "        inputs,\n",
    "        attention_mask=attention_mask,\n",
    "        max_new_tokens=3,\n",
    "        num_return_sequences=1\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0])\n",
    "    return extract_price(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_predict(prompt, device=\"cuda\"):\n",
    "    \"\"\"Weighted average of top K predictions\"\"\"\n",
    "    set_seed(42)\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    attention_mask = torch.ones(inputs.shape, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = fine_tuned_model(inputs, attention_mask=attention_mask)\n",
    "        next_token_logits = outputs.logits[:, -1, :].to('cpu')\n",
    "\n",
    "    next_token_probs = F.softmax(next_token_logits, dim=-1)\n",
    "    top_prob, top_token_id = next_token_probs.topk(TOP_K)\n",
    "    \n",
    "    prices, weights = [], []\n",
    "    for i in range(TOP_K):\n",
    "        predicted_token = tokenizer.decode(top_token_id[0][i])\n",
    "        probability = top_prob[0][i].item()\n",
    "        try:\n",
    "            price = float(predicted_token)\n",
    "            if price > 0:  # Filter invalid predictions\n",
    "                prices.append(price)\n",
    "                weights.append(probability)\n",
    "        except ValueError:\n",
    "            continue\n",
    "            \n",
    "    return sum(p * w for p, w in zip(prices, weights)) / sum(weights) if weights else 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "We assess model performance using:\n",
    "1. **Absolute Error (USD)**: |Prediction - True Price|\n",
    "2. **Squared Log Error (SLE)**: Penalizes relative errors\n",
    "3. **Accuracy Categories**:\n",
    "   - Green: Error < $40 or < 20%\n",
    "   - Yellow: Error < $80 or < 40%\n",
    "   - Red: Larger errors\n",
    "\n",
    "Comparison baselines:\n",
    "- GPT-4: $76 average error\n",
    "- Base LLaMA: $396 average error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceEvaluator:\n",
    "    def __init__(self, predictor, data, title=None, size=TEST_SIZE):\n",
    "        self.predictor = predictor\n",
    "        self.data = data\n",
    "        self.title = title or predictor.__name__.replace(\"_\", \" \").title()\n",
    "        self.size = size\n",
    "        self.results = {\n",
    "            'guesses': [],\n",
    "            'truths': [],\n",
    "            'errors': [],\n",
    "            'sles': [],\n",
    "            'colors': []\n",
    "        }\n",
    "\n",
    "    def _categorize_error(self, error, truth):\n",
    "        \"\"\"Classify prediction accuracy\"\"\"\n",
    "        if error < 40 or error/truth < 0.2:\n",
    "            return \"green\"\n",
    "        elif error < 80 or error/truth < 0.4:\n",
    "            return \"orange\"\n",
    "        return \"red\"\n",
    "\n",
    "    def evaluate_sample(self, index):\n",
    "        \"\"\"Run prediction on single test case\"\"\"\n",
    "        sample = self.data[index]\n",
    "        prediction = self.predictor(sample[\"text\"])\n",
    "        truth = sample[\"price\"]\n",
    "        error = abs(prediction - truth)\n",
    "        log_error = math.log(truth+1) - math.log(prediction+1)\n",
    "        sle = log_error ** 2\n",
    "        color = self._categorize_error(error, truth)\n",
    "        item_desc = sample[\"text\"].split(\"\\n\\n\")[1][:20] + \"...\"\n",
    "\n",
    "        # Store results\n",
    "        self.results['guesses'].append(prediction)\n",
    "        self.results['truths'].append(truth)\n",
    "        self.results['errors'].append(error)\n",
    "        self.results['sles'].append(sle)\n",
    "        self.results['colors'].append(color)\n",
    "\n",
    "        # Print colored output\n",
    "        print(f\"{COLOR_MAP[color]}{index+1}: \"\n",
    "              f\"Pred: ${prediction:,.2f} | \"\n",
    "              f\"True: ${truth:,.2f} | \"\n",
    "              f\"Error: ${error:,.2f} | \"\n",
    "              f\"SLE: {sle:,.2f} | \"\n",
    "              f\"Item: {item_desc}{RESET}\")\n",
    "\n",
    "    def visualize_results(self):\n",
    "        \"\"\"Generate prediction vs truth scatter plot\"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        max_val = max(max(self.results['truths']), max(self.results['guesses']))\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        plt.plot([0, max_val], [0, max_val], \n",
    "                color='deepskyblue', lw=2, alpha=0.6, \n",
    "                label='Perfect Prediction')\n",
    "        \n",
    "        # Actual predictions\n",
    "        plt.scatter(self.results['truths'], self.results['guesses'], \n",
    "                   s=3, c=self.results['colors'])\n",
    "        \n",
    "        plt.xlabel('Ground Truth Price ($)')\n",
    "        plt.ylabel('Model Prediction ($)')\n",
    "        plt.xlim(0, max_val)\n",
    "        plt.ylim(0, max_val)\n",
    "        plt.title(f\"{self.title}\\n\"\n",
    "                 f\"Avg Error: ${sum(self.results['errors'])/self.size:,.2f} | \"\n",
    "                 f\"Accuracy: {sum(1 for c in self.results['colors'] if c=='green')/self.size:.1%}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def run_evaluation(self):\n",
    "        \"\"\"Execute full evaluation pipeline\"\"\"\n",
    "        for i in range(self.size):\n",
    "            self.evaluate_sample(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Weighted Prediction Method\n",
    "\n",
    "Testing our fine-tuned model on {TEST_SIZE} test cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Evaluating Fine-Tuned Model ===\")\n",
    "evaluator = PriceEvaluator(weighted_predict, test, \"Fine-Tuned Model Performance\")\n",
    "evaluator.run_evaluation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Observations:\n",
    "1. Compare results to our baselines:\n",
    "   - GPT-4: $76 average error\n",
    "   - Base LLaMA: $396\n",
    "2. Green dots show accurate predictions\n",
    "3. Points above the blue line are overestimates\n",
    "4. Points below are underestimates\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
