{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### **What is Quantization?**\n",
        "Quantization reduces the precision of a model's weights and activations to use fewer bits, enabling:\n",
        "- **Smaller model size** (e.g., 4-bit vs 32-bit → 8x compression)\n",
        "- **Faster inference** (less memory bandwidth needed)\n",
        "- **Lower hardware requirements** (runs on consumer GPUs)\n",
        "\n",
        "### **Types of Quantization in LLMs**\n",
        "\n",
        "#### **1. Post-Training Quantization (PTQ)**\n",
        "Quantize a pre-trained model without retraining.\n",
        "\n",
        "| Type          | Bits | Key Features                          | Use Case                     |\n",
        "|---------------|------|---------------------------------------|------------------------------|\n",
        "| **FP16**      | 16   | Half-precision float                  | Baseline for comparisons     |\n",
        "| **INT8**      | 8    | Simple 8-bit integer                  | Balanced speed/accuracy      |\n",
        "| **NF4**       | 4    | 4-bit \"Normal Float\" (optimal bins)   | QLoRA fine-tuning            |\n",
        "| **GPTQ**      | 3-4  | Layer-wise calibration                | GPU inference                |\n",
        "\n",
        "```python\n",
        "# Example: 8-bit quantization\n",
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "```\n",
        "\n",
        "#### **2. Quantization-Aware Training (QAT)**\n",
        "Models are trained with simulated quantization.\n",
        "\n",
        "| Type          | Bits | Key Features                          |\n",
        "|---------------|------|---------------------------------------|\n",
        "| **QAT-FP8**   | 8    | Maintains float point                 |\n",
        "| **QAT-INT4**  | 4    | Simulates 4-bit during training       |\n",
        "\n",
        "#### **3. Hybrid Quantization**\n",
        "Combines different precisions:\n",
        "- **Weights**: 4-bit (e.g., NF4)\n",
        "- **Activations**: 8/16-bit\n",
        "```python\n",
        "# QLoRA hybrid example\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16  # <- Activations in bfloat16\n",
        ")\n",
        "```\n",
        "\n",
        "### **Key Tradeoffs**\n",
        "| Technique     | Memory Savings | Accuracy Drop | Hardware Support |\n",
        "|--------------|----------------|---------------|------------------|\n",
        "| FP16         | 2x             | None          | Universal        |\n",
        "| INT8         | 4x             | ~1-2%         | NVIDIA GPUs      |\n",
        "| NF4 (QLoRA)  | 8x             | ~2-5%         | Recent GPUs      |\n",
        "| GPTQ         | 10x+           | ~5-10%        | Consumer GPUs    |\n",
        "\n",
        "\n",
        "### **Why QLoRA Uses NF4**\n",
        "1. **Optimal binning**: Distributes 4-bit values to match float32 distribution\n",
        "2. **Double quantization**: Compresses quantization constants\n",
        "3. **bfloat16 compute**: Maintains stability during fine-tuning\n",
        "\n",
        "```python\n",
        "# Optimal QLoRA config\n",
        "BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",       # <- Special 4-bit type\n",
        "    bnb_4bit_use_double_quant=True,  # <- Extra compression\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Today's Learning Objectives:**\n",
        "1. Understand QLoRA and its advantages for efficient fine-tuning\n",
        "2. Compare different quantization approaches\n",
        "3. Analyze memory footprint of different model configurations\n",
        "4. Examine LoRA adapter architecture\n",
        "\n",
        "### **<--- Setup Section --->**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q datasets requests torch peft bitsandbytes transformers trl accelerate sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import with clear grouping\n",
        "import os\n",
        "import re\n",
        "import math\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# HuggingFace and Colab specific\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# PyTorch and Transformers\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed\n",
        ")\n",
        "from peft import LoraConfig, PeftModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Constants\n",
        "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
        "FINETUNED_MODEL = f\"ed-donner/pricer-2024-09-13_13.04.39\"\n",
        "\n",
        "# QLoRA Hyperparameters\n",
        "\n",
        "LORA_R = 32          # LoRA rank (dimension of the low-rank matrices)\n",
        "LORA_ALPHA = 64      # Scaling factor for LoRA weights\n",
        "TARGET_MODULES = [   # Which layers to apply LoRA to\n",
        "    \"q_proj\",        # Query projection\n",
        "    \"v_proj\",        # Value projection\n",
        "    \"k_proj\",        # Key projection\n",
        "    \"o_proj\"         # Output projection\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Before proceeding, you'll need:\n",
        "1. A HuggingFace account (https://huggingface.co)\n",
        "2. An access token (create at https://huggingface.co/settings/tokens)\n",
        "3. Add token to Colab secrets (Key icon → New secret) named 'HF_TOKEN'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Quantization Comparison Section\n",
        "\n",
        "### Quantization Comparison\n",
        "\n",
        "We'll compare three configurations:\n",
        "1. No quantization (full precision)\n",
        "2. 8-bit quantization\n",
        "3. 4-bit quantization (QLoRA)\n",
        "\n",
        "Note: After each full model load, you'll need to:\n",
        "Runtime → Restart session → Run initial cells again\n",
        "to clear GPU memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. No Quantization\n",
        "\n",
        "### Base Model (No Quantization)\n",
        "- Full 32-bit precision\n",
        "- Maximum memory usage\n",
        "- Best theoretical performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. 8-bit Quantization\n",
        "\n",
        "### 8-bit Quantization\n",
        "- Reduces memory usage significantly\n",
        "- Minimal accuracy loss\n",
        "- Good balance for many applications\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(load_in_8bit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.1f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. 4-bit QLoRA\n",
        "\n",
        "### 4-bit QLoRA Configuration\n",
        "- Most memory efficient\n",
        "- Uses 'nf4' (normal float 4) quantization\n",
        "- Double quantization for additional savings\n",
        "- bfloat16 compute dtype for stability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=quant_config,\n",
        "    device_map=\"auto\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Memory footprint: {base_model.get_memory_footprint() / 1e9:,.2f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "base_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-tuned Model Loading\n",
        "\n",
        "## Loading Fine-tuned Adapters\n",
        "The PeftModel combines:\n",
        "1. Original quantized base model\n",
        "2. Trained LoRA adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fine_tuned_model = PeftModel.from_pretrained(base_model, FINETUNED_MODEL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Memory footprint with adapters: {fine_tuned_model.get_memory_footprint() / 1e9:,.2f} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fine_tuned_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LoRA Architecture Analysis\n",
        "\n",
        "### How LoRA Adapts Pretrained Models\n",
        "\n",
        "LoRA (Low-Rank Adaptation) works by injecting **trainable low-rank matrices** into specific layers while keeping the original weights frozen. This is more efficient than full fine-tuning because:\n",
        "\n",
        "1. Only ~0.1-1% of parameters are updated\n",
        "2. Original model remains intact (no catastrophic forgetting)\n",
        "3. Adapters can be swapped for different tasks\n",
        "\n",
        "\n",
        "Each target module has two low-rank matrices:\n",
        "- lora_A (dimension: original_size × r)\n",
        "- lora_B (dimension: r × original_size)\n",
        "\n",
        "Where r = LORA_R (32 in our case)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate adapter parameters for one layer\n",
        "lora_q_proj = 4096 * 32 + 4096 * 32  # (input_dim × r) + (r × output_dim)\n",
        "lora_k_proj = 4096 * 32 + 1024 * 32\n",
        "lora_v_proj = 4096 * 32 + 1024 * 32\n",
        "lora_o_proj = 4096 * 32 + 4096 * 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lora_layer = lora_q_proj + lora_k_proj + lora_v_proj + lora_o_proj\n",
        "total_params = lora_layer * 32  # 32 layers in the model\n",
        "size_mb = (total_params * 4) / 1_000_000  # 4 bytes per parameter (float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Total number of params: {params:,} and size {size:,.1f}MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaways:\n",
        "1. QLoRA enables efficient fine-tuning with minimal memory overhead\n",
        "2. 4-bit quantization reduces memory by ~8x compared to full precision\n",
        "3. LoRA adapters add only ~10MB of parameters while being effective"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llms",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
