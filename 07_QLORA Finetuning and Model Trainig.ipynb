{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Price Prediction with QLoRA Fine-Tuning\n",
    "\n",
    "### Learning Objectives:\n",
    "1. Configure QLoRA for efficient fine-tuning\n",
    "2. Implement supervised fine-tuning with PEFT\n",
    "3. Monitor training with W&B\n",
    "4. Save and share trained adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required packages (commented to prevent accidental execution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets requests torch peft bitsandbytes transformers trl accelerate sentencepiece wandb matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import with clear grouping\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# HuggingFace and Colab specific\n",
    "from google.colab import userdata\n",
    "from huggingface_hub import login\n",
    "import wandb\n",
    "\n",
    "# PyTorch and Transformers\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    set_seed\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Configuration\n",
    "\n",
    "Key settings for our fine-tuning experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and Data\n",
    "BASE_MODEL = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "HF_USER = \"ed-donner\"\n",
    "DATASET_NAME = f\"{HF_USER}/pricer-data\"\n",
    "MAX_SEQUENCE_LENGTH = 182\n",
    "\n",
    "# Run Management\n",
    "RUN_NAME = f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"\n",
    "PROJECT_NAME = \"pricer\"\n",
    "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"\n",
    "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"\n",
    "\n",
    "# --- QLoRA Hyperparameters ---\n",
    "\"\"\"\n",
    "### QLoRA Configuration\n",
    "\n",
    "Parameters for efficient fine-tuning:\n",
    "\"\"\"\n",
    "LORA_R = 32          # Rank of low-rank adaptation matrices\n",
    "LORA_ALPHA = 64      # Scaling factor for LoRA weights\n",
    "TARGET_MODULES = [   # Which attention layers to adapt\n",
    "    \"q_proj\",        # Query projections\n",
    "    \"v_proj\",        # Value projections\n",
    "    \"k_proj\",        # Key projections\n",
    "    \"o_proj\"         # Output projections\n",
    "]\n",
    "LORA_DROPOUT = 0.1   # Dropout probability for LoRA layers\n",
    "QUANT_4_BIT = True   # Use 4-bit quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Hyperparameters\n",
    "\n",
    "#### Training Configuration\n",
    "\n",
    "Optimization settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 1                    # Training cycles through dataset\n",
    "BATCH_SIZE = 4                # Samples per batch\n",
    "GRADIENT_ACCUMULATION_STEPS = 1  # Effective batch size = BATCH_SIZE * STEPS\n",
    "LEARNING_RATE = 1e-4          # Initial learning rate\n",
    "LR_SCHEDULER_TYPE = 'cosine'  # Learning rate schedule\n",
    "WARMUP_RATIO = 0.03           % of steps for LR warmup\n",
    "OPTIMIZER = \"paged_adamw_32bit\"  # Memory-efficient AdamW variant\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "STEPS = 50                    # Log metrics every N steps\n",
    "SAVE_STEPS = 2000             # Save checkpoint every N steps\n",
    "LOG_TO_WANDB = True           # Enable Weights & Biases logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HuggingFace and W&B Login\n",
    "\n",
    "Required for model access and experiment tracking:\n",
    "1. Get your HF token from https://huggingface.co/settings/tokens\n",
    "2. Get your W&B API key from https://wandb.ai/settings\n",
    "3. Add both to Colab secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to HuggingFace\n",
    "\n",
    "hf_token = userdata.get('HF_TOKEN')\n",
    "login(hf_token, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Weights & Biases\n",
    "wandb_api_key = userdata.get('WANDB_API_KEY')\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
    "wandb.login()\n",
    "\n",
    "# Configure Weights & Biases to record against our project\n",
    "os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\" if LOG_TO_WANDB else \"end\"\n",
    "os.environ[\"WANDB_WATCH\"] = \"gradients\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing Dataset\n",
    "\n",
    "Our price prediction dataset contains:\n",
    "- Product descriptions\n",
    "- Corresponding prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(DATASET_NAME)\n",
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Base Model with QLoRA\n",
    "\n",
    "Using 4-bit quantization for memory efficiency:\n",
    "- NF4 quantization type\n",
    "- Double quantization for additional savings\n",
    "- bfloat16 compute dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick the right quantization\n",
    "\n",
    "if QUANT_4_BIT:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    "  )\n",
    "else:\n",
    "  quant_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.bfloat16\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL,\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "print(f\"\\nMemory footprint: {base_model.get_memory_footprint() / 1e6:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring the Training Process\n",
    "\n",
    "We use two key configurations:\n",
    "1. LoRA parameters for efficient adaptation\n",
    "2. Training arguments for the optimization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA Configuration: \n",
    "lora_parameters = LoraConfig(\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    r=LORA_R,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=TARGET_MODULES,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Configuration: General configuration parameters for training\n",
    "\n",
    "train_parameters = TrainingArguments(\n",
    "    output_dir=PROJECT_RUN_NAME,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    optim=OPTIMIZER,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    weight_decay=0.001,\n",
    "    max_grad_norm=0.3,\n",
    "    warmup_ratio=WARMUP_RATIO,\n",
    "    lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
    "    logging_steps=STEPS,\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=10,\n",
    "    bf16=True,\n",
    "    group_by_length=True,\n",
    "    report_to=\"wandb\" if LOG_TO_WANDB else None,\n",
    "    run_name=RUN_NAME,\n",
    "    max_seq_length=MAX_SEQUENCE_LENGTH,\n",
    "    save_strategy=\"steps\",\n",
    "    hub_strategy=\"every_save\",\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=HUB_MODEL_NAME,\n",
    "    hub_private_repo=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator Setup\n",
    "\n",
    "Ensures the model only learns to predict prices (not descriptions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_template = \"Price is $\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Fine-Tuning\n",
    "\n",
    "The SFTTrainer will:\n",
    "1. Apply LoRA adapters to the base model\n",
    "2. Train only the adapter parameters\n",
    "3. Log progress to W&B\n",
    "4. Save checkpoints periodically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=train,\n",
    "    peft_config=lora_parameters,\n",
    "    args=train_parameters,\n",
    "    data_collator=collator,\n",
    "    dataset_text_field=\"text\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting training run: {RUN_NAME}\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Sharing Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "trainer.model.push_to_hub(PROJECT_RUN_NAME, private=True)\n",
    "print(f\"\\nModel saved to Hub: {HUB_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up W&B\n",
    "if LOG_TO_WANDB:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
